{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "569ad5f6",
   "metadata": {},
   "source": [
    "# 6. Scikit-learn (sklearn)\n",
    "\n",
    "scikit-learn의 장점 중 하나는, 사용하기 편하다는 것이다. \n",
    "어느 알고리즘이던, .fit, .fit_transform, .predict가 통일되서 작동되기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd841a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LingearREgression\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr.fit(X,y)\n",
    "p = lr.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b22f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logstic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "# log_loss 분류학습 평가/학습용. roc_auc_score 분류 모델 평가요\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X,y)\n",
    "p = lr.predict_proba(X)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8167f924",
   "metadata": {},
   "source": [
    "분류학습에서 predict_proba라는 함수가 있는데, 이는 target_col의 타입을 0,1로 반환하는 것이 아닌, 각 타입에 대해 샘플의 확률을 반환한다.\n",
    "\n",
    "예를 들어, target_col이 5개의 class를 갖고 있다면, 5개의 열에 대해서 하나의 샘플의 행이 각각, 확률을 갖게 된다.\n",
    "\n",
    "[0.1, 0.6, 0.1, 0.2, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46fc49cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clustering\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "# Kmeans는 cluster를 생성하여, 중심축을 기준으로 유클리드 계산법으로 각 샘플에 대한\n",
    "# 절대 거리를 계산하여, 군집을 형성하는 알고리즘이다. -> 클러스터\n",
    "\n",
    "import numpy as np\n",
    "X = np.array([[1,2],[1,4],[1,0],[10,2],[10,4],[10,0]])\n",
    "KMeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "KMeans.labels_\n",
    "KMeans.predict([[0,0],[12,3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4491a8",
   "metadata": {},
   "source": [
    "clustering 알고리즘은 비지도학습 알고리즘이기 떄문에, 종속 변수 y가 없습니다. fit 함수에 입력값만으로 입력하여, 학습을 진행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f676285c",
   "metadata": {},
   "source": [
    "scikit-learn에서 주로 사용하는 알고리즘입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47f335d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a29a02d",
   "metadata": {},
   "source": [
    "선형 모델로는 리니어 리그레션, 로지스틱 리그레션 알고리즘을 주로 사용합니다.\n",
    "\n",
    "트리 기반 모델로는 디시젼트리 리그레션 / 클래시파이어를 사용합니다.\n",
    "\n",
    "데이터 과학 대회에서 주로 사용되는 알고리즘은 ensemble에서 제공하는 랜덤포레스트리그레서, 랜덤포레스트 클래시파이어 그리고 그래디언트 부스팅 리그레서, 그레디언트 부스팅 클래시파이어가 되겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e130d170",
   "metadata": {},
   "source": [
    "## 6,2 변수 변환\n",
    "scikit-learn에서 제공하는 주요 변수 변환 기능을 확인해보겠습니다.\n",
    "\n",
    "1. 수치형 변수에 적용 가능한 변환 방법 \n",
    "\n",
    "- MinMaxScaler - 최댓값과 최솟값을 1과 0으로 변환\n",
    "- StandardScaler - 정규분포로 변환\n",
    "- RobustScaler - 이상치의 영향을 덜 받는 변환 방법\n",
    "- QuantileTransformer - 백분위수 정보를 활용해 변환\n",
    "- PowerTransformer - yeo-johnson 또는 box-box 방법으로 정규분포화하는 방법\n",
    "\n",
    "2. 범주형 변수에 적용 가능한 변환 방법\n",
    "- LabelEncoder : 0부터 N까지의 숫자로 각 범주를 변환, 종속변수에 사용\n",
    "- OneHotEncoder : 각 범주를 이진 변수로 변환해 0과 1로만 범주를 표현\n",
    "- OrdinalEncoder : 0부터 N까지의 숫자로 각 범주를 변환, 독립 변수에 사용.\n",
    "\n",
    "Q. onehotencoder vs get_dummies\n",
    "- get_dummies -> pandas library : df형식에서 작동. eda에 유용\n",
    "- onehotencoder -> scikit-learn library : ndarray 형식에서 작동.\n",
    "\n",
    "3. 차원 축소시 사용하는 비지도 알고리즘 \n",
    "- PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a88fbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(df[num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc36813a",
   "metadata": {},
   "source": [
    "##### scikit-learn의 텍스트 데이터를 숫자로 변환하는 함수\n",
    "- 참고할 것. 나중에 다시 배우게 됨\n",
    "- countVectorizer : 입력 테스트에 있는 모든 단어들을 개별 변수로 보고, 각 단어의 등장 횟수를 세어 수치화\n",
    "\n",
    "- TfidVectorizer : Term frequency inverse document frequency 값을 곱해서 수치화 하는 방법. 출몰한 빈도와 한 단어가 전체 문서에서 등장한 횟수를 역수 취한 값. 자주 등장하는 단어인 경우, 단어의 중요도를 낮추고, 특정 문서에서 자주 등장하는 단어인 경우, 중요도를 높임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f15329a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "  'This is the first document',\n",
    "  'This document is the second document',\n",
    "  'And this is the third one.',\n",
    "  'Is this the first document?'\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812cb4ae",
   "metadata": {},
   "source": [
    "### 6.3 평가 지표 Metric\n",
    "\n",
    "Scikit-learn에서는 모델 평가를 위한, 다양한 평가 지표를 제공합니다. 분류 문제에서 자주 사용되는 log_loss, roc_acu_score, confusion_matrix가 사용됩니다. 아래는 confusion_matrix를 사용한 예시입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25765e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss, roc_auc_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fc33cd",
   "metadata": {},
   "source": [
    "1. log_loss : mean_squared_error(log_loss_function)\n",
    "2. roc_auc_score : ROC 곡선 아래 면적. 범위 0~1. 1에 가까울수록 좋음.\n",
    "  - 양성 샘플이 음성 샘플보다 높은 점수를 받을 확률.\n",
    "3. accuracy_score : 전체 예측 중 맞춘 비율.\n",
    "  Accuracy = TP + TN / TP + TN + FP + FN\n",
    "  직관적이고 쉬움. 클래스 불균형일 때 잘못된 판단을 줄 수 있음.\n",
    "4. confusion_matrix : 실제값도 예측값의 조합을 매트릭스로 보여줌.   Accuracy의 backwork를 매트릭스로 보여주는 것,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a9f74f",
   "metadata": {},
   "source": [
    "confusion_matrix는 postive-negative, negative-negatice 등을 확인하기에 유용합니다.\n",
    "\n",
    "회귀 문제에서 자주 사용되는 평가 지표는 mean_squared_error, mean_absolute_error 그리고 r2_score이며 각각 sklearn.metrics 모듈에서 불러올 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83506c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dcb555",
   "metadata": {},
   "source": [
    "## 검증 Validation\n",
    "\n",
    "가장 많이 사용되는 함수는 train_test_split, KFold 그리고 KFold, StratifiedKFold입니다.\n",
    "\n",
    "tran_test_split은 학습 데이터셋의 일부를 train과 test로 나눠서, 학습과 검증을 하는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6b88730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150,)\n",
      "(90, 4) (60, 4) (90,) (60,)\n"
     ]
    }
   ],
   "source": [
    "#train_test_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedGroupKFold\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X, y = load_iris(return_X_y= True)\n",
    "print(X.shape, y.shape)\n",
    "X_trn, X_tst, y_trn, y_tst = train_test_split(X,y, test_size = .4, random_state = 0)\n",
    "print(X_trn.shape, X_tst.shape, y_trn.shape, y_tst.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d54a6d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f6fff4e",
   "metadata": {},
   "source": [
    "X_trn, X_tst, y_trn, y_tst = train_test_split(X,y, test_size = .4, random_state = 0)\n",
    "\n",
    "train_test_split의 인자로 X와 y를 입력한 후, 검증 데이터로 사용하고자 하는 비율을 입력합니다.(40%). 그렇게 되면 각각의 X와 y를 훈련과 테스트셋을 6:4 비율로 나눠서, X_훈련, X_검증, y_훈련, y_검증으로 나누게 됩니다.\n",
    "\n",
    "| 데이터 과학 대회에 팀 단위로 참여시, 팀원 간의 실험 재현이 될 수 있도록 random_state를 고정해두는 것이 필요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a53d2e",
   "metadata": {},
   "source": [
    "##### KFold\n",
    "\n",
    "KFold는 모든 데이터셋을 훈련에도, 검증에도 사용할 수 있게끔 하는 알고리즘입니다. 평균적인 모델의 성능을 평가하기에도 좋습니다. 각각의 폴드에서 실행한 예측값을 모두 다 모아, 전체 데이터에 대한 예측값을 받을 수 있습니다. (CV prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75b4c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=3, shuffle=True,random_state=42)\n",
    "\n",
    "p_val = np.zeros((trn.shape[0], n_class))\n",
    "p_tst = np.zeros((tst.shape[0], n_class))\n",
    "# validation셋에 대한 예측값 저장용 어레이 -> trn.shape[0], 학습셋 샘플 수, n_class -> 클래스 갯수\n",
    "# tst셋에 대한 예측값 저장용 어레이\n",
    "\n",
    "for i, (i_trn, i_val) in enumerate(cv.split(trn), 1):\n",
    "# enumerate -> iteratable 객체에서 매 for loop에서 반환할 때마다, index 또한 같이 반환.\n",
    "# 생성된 cv 객체를 통해, trn을 학습, 검증셋으로 나눔, 1인자는 몇 번 할 때마다 shuffle할 것인지\n",
    "  print(f'training model for CV #{i}')\n",
    "  lgb_trn = lgb.Dataset(trn[i_trn], y[i_trn], feature_name = feature_name)\n",
    "  lgb_val = lgb.Dataset(trn[i_val], y[i_val], feature_name = feature_name)\n",
    "  \n",
    "  clf = lgb.train(params,\n",
    "                  lgb_trn,\n",
    "                  num_boost_round = n_est,\n",
    "                  early_stopping_rounds = n_stop,\n",
    "                  valid_sets = lgb_val,\n",
    "                  verbose_eval = 100)\n",
    "  # 지금은 인자들의 이름이 조금씩 바뀌었을 것. 파이썬의 버전에 따라.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2fe804",
   "metadata": {},
   "source": [
    "### 결정트리\n",
    "\n",
    "- RandomForest와 LightGBM의 기반이 되는 알고리즘.\n",
    "- ㄹㅇ 나무 마냥, 줄줄이 잎사귀 타고 내려가서 학습ㅎ라는 방법.\n",
    "- 작동방식은 맨 위의 노드에서 어떤 피쳐로 가장 잘 분류하는지 (purity, entropy, information 이라고도 표현)를 판단함. 예를 들어, 첫번째 노드에서 6:4로 분류하는 것과 1:9로 분류하는 것이 있다면, 당연 6:4로 분류하는 것이 정보 습득이 더욱 많음.(더욱 순수하게 걸러냄. 또는 얻는 정보가 더욱 많다라고 판단할 수 있음. 예를 들어, 내가 1~100까지의 숫자를 정하고, 상대방이 내 숫자를 질문을 통해, 맞추는 게임을 한다고 생각해보자. 50 이상인가, 이하인가를 묻는 질문이 10이하인가 10 이상인가를 묻는 질문보다 얻을 수 있는 정보가 (일반적으로 또는 확률적으로) 더 크다)\n",
    "- 좀 더 자세하게는, 0, 1 binary classfication에서 각 노드의 순수도를 계산. \n",
    "\n",
    "- 결정트리의 특징으로는 분류 문제와 회귀 문제 모두 적용 가능하며, 모델 해석에 용이하다. 시각화를 통해 어떤 로직으로 결과를 판단했는지 알 수 있음. 또한, 스케일에 영향을 받지 않는다. **스케일이 달라도 동일한 결과가 나오므로, 데이터 전처리시 스케일링을 적용하지 않아도 무관. 또한, 범주형 변수를 수치형 변수로 변화하지 않아도 학습이 가능한 알고리즘.**\n",
    "\n",
    "- 결정트리를 분류 모델로 학습하는 방법과 회귀 모델로 학습하는 방법이 있는데, 두 가지 학습 방법 모두, 특정 노드에서 손실함수를 최소화하는 변수와 기준값을 탐색. 분류 모델에서는 Gini 또는 Entropy 손실함수를 정의하고, 회귀 모델에서는 MSE 또는 MAE를 손실함수를 정함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39338e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e521202",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff8f3702",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f524c00",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1b1096",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
